# How to Configure ollm.chat

To use ollm.chat with an OpenAI-compatible API server, you'll need to follow these steps:

1. **API Base URL Setup**:
    - If you are using the default OpenAI base URL, you need to set the API URL to "https://api.openai.com". 
    - If you are using a self-hosted Language Model, please set the API URL to point to your local server's address, which will be in the format: "http://IP:PORT/".

2. **API Key Configuration**:
    - For the default OpenAI base URL, you'll need to obtain an API key from [OpenAI's platform](https://platform.openai.com/). Once you have the API key, make sure to set it for authentication.
    - If you are connecting to a local server (self-hosted Language Model), no API key is required. Some local servers might require a random string as an API Key.

3. **Max Tokens**:
    - The "Max Tokens" parameter controls the maximum length of the response generated by the model.
    - If you are using OpenAI, ensure that you set the "Max Tokens" value to a number greater than 1. 
    - If you are using a local server, use the default value -1 for infinite or a number greater than 1.

4. **Local Server Setup** (For Self-hosted Language Model):
    - Download and Setup LM Studio from [https://lmstudio.ai](https://lmstudio.ai) on your PC or server.
    - Run the local server provided by "LM Studio" to make it accessible at a specific IP and port.

These instructions should help you configure the API settings for our app and get it ready for use with your desired language model, whether it's the default OpenAI setup or a self-hosted solution.
